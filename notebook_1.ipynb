{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install neo4j --user\n",
    "! pip install pandas --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import sqlite3;\n",
    "import pandas as pd;\n",
    "from pandas.io.sql import DatabaseError\n",
    "import os;\n",
    "from sqlite3 import OperationalError\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIGURATION ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "CSV_SUB_DIR=\"entities\"\n",
    "IMPORT_NEO_FOLDER=\"C:/Users/39391/.Neo4jDesktop/relate-data/dbmss/dbms-c4c5ade6-22d3-4366-8b33-f83252bd7d2c/import/\"; #example path\n",
    "SQLITE_DB_NAME=\"June 21/output UK/results.sqlite\";\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Preparing CSV files\n",
    "\n",
    "First part of the migration procedure. It consists in preparing a set of CSV files that will be imported in the Neo4j db in the second part.\n",
    "The program create a CSV file for each entities (nodes and relationship) of the future graph. Above there are 3 variables to configure:\n",
    "\n",
    "- **SQLITE_DB_NAME** : the name of the SQLite database (that will be fetched from the current directory)\n",
    "- **IMPORT_NEO_FOLDER** : Absolute path of the directory where CSV will be stored. Neo4j security has a default setting that local files can only be read from the Neo4j import directory, which is different based on the used operating system. [In Neo4j doc](https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/#query-load-csv-introduction), it is explained how to change these settings and which are the default import directories.\n",
    "- **CSV_SUB_DIR** : the name of the subdirectory where CSV will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "connector = sqlite3.connect(SQLITE_DB_NAME);\n",
    " \n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "    \n",
    "class Log:\n",
    "\n",
    "    def error(type,name,message):\n",
    "        print(\"[\"+datetime.now().strftime(\"%H:%M:%S\")+\"]\"+\"[ERROR][\"+type+\"]\"+\"[\"+name+\"] \" +message);\n",
    "\n",
    "    def warn(type,name,message):\n",
    "        print(\"[\"+datetime.now().strftime(\"%H:%M:%S\")+\"]\"+\"[WARN][\"+type+\"]\"+\"[\"+name+\"] \" +message);\n",
    "\n",
    "def get_list_from_sqlite(query,connector,entity_name):\n",
    "    try:\n",
    "        cursor=connector.cursor();\n",
    "        cursor.execute(query);\n",
    "        l=cursor.fetchall();\n",
    "        cursor.close();\n",
    "        return l;\n",
    "    except OperationalError as e:\n",
    "        Log.warn(\"GENERIC\",entity_name,\"A problem occur while searching for other elements of this type.\");\n",
    "        return list();\n",
    "        \n",
    "\n",
    "def get_data_frame_from_list(list,headers=None):\n",
    "    if len(list)==0:\n",
    "        return pd.DataFrame([]);\n",
    "    df=pd.DataFrame(list);\n",
    "    if headers!=None:\n",
    "        df.columns=headers;\n",
    "    return df;\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From SQLite to CSV tables (Part 1)\n",
    "<span style=\"color:#9e9e9e\"> Exporting WEB_SITE, LANDING_NAME, NETWORK nodes and LAND,LOCATED, CNAME relationships</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating tables (pandas dataframe) that fully describe each node and relationship type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for NODES: WEB_SITE(url_id,domain_name_id), LANDING_NAME(domain_name,landing_url,landing_https)\n",
    "query_node_web_site = \"SELECT ws.url_id, wsdn.domain_name_id FROM web_site ws INNER JOIN web_site_domain_name wsdn on ws.url_id = wsdn.web_site_id WHERE url_id IS NOT NULL\";\n",
    "query_node_landing_name= \"SELECT DISTINCT  ws.name_id AS domain_name, wsl.landing_url, wsl.landing_https FROM web_site_lands wsl LEFT JOIN web_server ws on ws.name_id = wsl.web_server_id WHERE landing_url IS NOT NULL\"\n",
    "query_node_network =\"SELECT DISTINCT iad.ip_network_id, COALESCE(r.state,'-') AS state FROM ip_address_depends iad LEFT JOIN ip_range_rov irr on iad.ip_range_rov_id = irr.compressed_notation LEFT JOIN prefixes_table pt on irr.compressed_notation = pt.ip_range_rov_id LEFT JOIN rov r on r.id = pt.rov_id\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for RELATIONSHIPS: LAND (from WEB_SITE to LANDING_NAME) and LOCATED (from LANDING_NAME to NETWORK) relationships\n",
    "query_rel_land = \"SELECT ws.url_id, wsl.starting_https, wsl.landing_https, wsl.landing_url FROM web_site ws INNER JOIN web_site_lands wsl ON ws.url_id=wsl.web_site_id WHERE wsl.landing_url IS NOT NULL;\";\n",
    "query_rel_located_ln =\"SELECT ws.name_id,iad.ip_address_id, iad.ip_network_id FROM web_site_lands wsl INNER JOIN web_server ws ON wsl.web_server_id=ws.name_id INNER JOIN domain_name dn on ws.name_id = dn.string INNER JOIN access a on dn.string = a.domain_name_id INNER JOIN ip_address ia on a.ip_address_id = ia.exploded_notation INNER JOIN ip_address_depends iad on ia.exploded_notation = iad.ip_address_id\";\n",
    "query_rel_belong_ln=\"SELECT ws.name_id AS ln_name, z.name AS ln_zone FROM web_site_lands wsl INNER JOIN web_server ws on wsl.web_server_id = ws.name_id INNER JOIN domain_name dn on ws.name_id = dn.string INNER JOIN direct_zone dz on dn.string = dz.domain_name_id INNER JOIN zone z on dz.zone_id = z.name\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving results in pandas dataframes\n",
    "\n",
    "#NODES:\n",
    "try:\n",
    "    df_node_web_site = pd.read_sql_query(query_node_web_site, connector);\n",
    "except DatabaseError as e:\n",
    "    Log.error(\"NODE\",\"WEB_SITE\",\"An error occur while loading the current entity. Cannot continue.\");\n",
    "    raise StopExecution;\n",
    "try:\n",
    "    df_node_landing_name = pd.read_sql_query(query_node_landing_name, connector);\n",
    "except DatabaseError as e:\n",
    "    Log.error(\"NODE\",\"LANDING_NAME\",\"An error occur while loading the current entity. Cannot continue.\");\n",
    "    raise StopExecution;\n",
    "try:\n",
    "    df_node_network = pd.read_sql_query(query_node_network,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.error(\"NODE\",\"NETWORK\",\"An error occur while loading the current entity. Cannot continue.\");\n",
    "    raise StopExecution;\n",
    "\n",
    "#RELATIONSHIP:\n",
    "try:\n",
    "    df_rel_land = pd.read_sql_query(query_rel_land,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"LAND\",\"Cannot load the current relationship..\");\n",
    "try:\n",
    "    df_rel_located_ln = pd.read_sql_query(query_rel_located_ln,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"LOCATED_LANDING_NAME\",\"Cannot load the current relationship..\");\n",
    "try:\n",
    "    df_rel_belong_ln=pd.read_sql_query(query_rel_belong_ln,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"BELONG_LANDING_NAME\",\"Cannot load the current relationship..\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep search for other LANDING_NAME nodes with relative CNAME and LOCATED relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for the first search: search from already-founded LANDING_NAME nodes for RR CNAME, obtaining new LANDING_NAME nodes:\n",
    "query_nodes_ln =\" SELECT DISTINCT  a.alias_id AS domain_name, \\\"-\\\" AS landing_url, wsl.landing_https FROM web_site_lands wsl INNER JOIN web_server ws ON wsl.web_server_id=ws.name_id INNER JOIN domain_name dn ON ws.name_id=dn.string INNER JOIN alias a ON a.name_id=dn.string;\"\n",
    "query_rel_cname =\"SELECT  ws.name_id AS name_1, a.alias_id AS name_2 FROM web_site_lands wsl INNER JOIN web_server ws ON wsl.web_server_id=ws.name_id INNER JOIN domain_name dn ON ws.name_id=dn.string INNER JOIN alias a ON a.name_id=dn.string;\";\n",
    "#Get founded LANDING_NAME nodes and CNAME relationships in 2 lists:\n",
    "init_nodes_ln_list = get_list_from_sqlite(query_nodes_ln,connector,\"LANDING_NAME\");\n",
    "init_rel_cname = get_list_from_sqlite(query_rel_cname,connector,\"CNAME\");\n",
    "#Set also an empty list that will contain LOCATED and BELONG relationships for all the new LANDING_NAME nodes\n",
    "init_rel_located =list();\n",
    "init_rel_belong = list();\n",
    " \n",
    "#Set a buffer list containing the new LANDING_NAME nodes. In this loop we will deep search all new LANDING_NAMES founded from CNAME RR:\n",
    "buffer_list_of_ln_nodes = init_nodes_ln_list.copy();\n",
    "\n",
    "while len(buffer_list_of_ln_nodes)>0:\n",
    "    node= buffer_list_of_ln_nodes.pop();\n",
    "    #From each new LANDING_NAME node, search for another LANDING_NAME node (and CNAME rel) caused by a CNAME RR (searching in alias table)\n",
    "    query_nodes_ln= \"SELECT a.alias_id AS domain_name, \\\"-\\\" AS landing_url,\\\"{}\\\" AS landing_https  FROM alias a WHERE a.name_id=\\\"{}\\\"\".format(node[2], node[0]);\n",
    "    query_rel_cname=\"SELECT \\\"{}\\\" AS name_1, al.alias_id AS name_2  FROM alias al WHERE al.name_id=\\\"{}\\\"\".format(node[0], node[0]);\n",
    "    #From each new LANDING_NAME, search for LOCATED (from LANDING_NAME to NETWORK relationship) (i.e. check if there's a record in the access table):\n",
    "    query_rel_located=\" SELECT a.alias_id AS name_id,iad.ip_address_id,  iad.ip_network_id  FROM alias a INNER JOIN domain_name dn on a.alias_id = dn.string INNER JOIN access a on dn.string = a.domain_name_id INNER JOIN ip_address ia on a.ip_address_id = ia.exploded_notation INNER JOIN ip_address_depends iad on ia.exploded_notation = iad.ip_address_id WHERE a.alias_id=\\\"{}\\\";\".format(node[0]);\n",
    "    #From each new LANDING_NAME, search for BELONG (from LANDING_NAME to ZONE relationship):\n",
    "    query_rel_belong=\"SELECT a.alias_id AS ln_name, z.name AS ln_zone from alias a INNER JOIN domain_name dn ON a.alias_id = dn.string INNER JOIN direct_zone dz ON dn.string = dz.domain_name_id INNER JOIN zone z ON dz.zone_id = z.name where a.alias_id=\\\"{}\\\";\".format(node[0]);\n",
    "    #Add founded LANDING_NAME nodes in the buffer list, in order to check in next iterations if there are others CNAME RR:\n",
    "    buffer_list_of_ln_nodes=buffer_list_of_ln_nodes +get_list_from_sqlite(query_nodes_ln,connector,\"LANDING_NAME\");\n",
    "    #Filter buffer_list in order to eventually remove empty rows:\n",
    "    buffer_list_of_ln_nodes = list(filter(None, buffer_list_of_ln_nodes));\n",
    "    #Update initial lists containing LANDING_NAMES, CNAME, BELONG and LOCATED entities:\n",
    "    init_nodes_ln_list.extend(get_list_from_sqlite(query_nodes_ln,connector,\"LANDING_NAME\"));\n",
    "    init_rel_cname.extend(get_list_from_sqlite(query_rel_cname,connector,\"CNAME\"));\n",
    "    init_rel_located.extend(get_list_from_sqlite(query_rel_located,connector,\"LOCATED\"));\n",
    "    init_rel_belong.extend(get_list_from_sqlite(query_rel_belong,connector,\"BELONG\"));\n",
    "\n",
    " \n",
    "\n",
    "#Filter final lists in order to eventually remove empty rows:\n",
    "init_nodes_ln_list = list(filter(None, init_nodes_ln_list));\n",
    "init_rel_cname = list(filter(None, init_rel_cname));\n",
    "init_rel_located = list(filter(None, init_rel_located));\n",
    "init_rel_belong = list(filter(None,init_rel_belong));\n",
    "\n",
    "#Store results in pandas dataframes:\n",
    "df_rel_cname_ln= get_data_frame_from_list(init_rel_cname,[\"name_1\",\"name_2\"]);\n",
    "df_node_landing_name=pd.concat([df_node_landing_name,get_data_frame_from_list(init_nodes_ln_list,[\"domain_name\",\"landing_url\",\"landing_https\"])]);\n",
    "df_rel_located_ln=pd.concat([df_rel_located_ln,get_data_frame_from_list(init_rel_located,[\"name_id\",\"ip_address_id\",\"ip_network_id\"])]);\n",
    "df_rel_belong_ln  =pd.concat([df_rel_belong_ln,get_data_frame_from_list(init_rel_belong,[\"ln_name\",\"ln_zone\"])]);\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting entities in CSV files (in the directory specified in configuration strings):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting nodes and relationships in csv files\n",
    "\n",
    "os.makedirs(IMPORT_NEO_FOLDER+CSV_SUB_DIR, exist_ok=True);\n",
    "\n",
    "try:\n",
    "    #Nodes:\n",
    "    df_node_web_site.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/node_web_site.csv\", index=False);\n",
    "    df_node_landing_name.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/node_landing_name.csv\", index=False);\n",
    "    df_node_network.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/node_network.csv\", index=False);\n",
    "\n",
    "    #Relationships:\n",
    "\n",
    "    df_rel_land.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_land.csv\", index=False);\n",
    "    df_rel_cname_ln.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_cname_ln.csv\", index=False);\n",
    "    df_rel_located_ln.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_located_ln.csv\", index=False);\n",
    "    df_rel_belong_ln.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_belong_ln.csv\", index=False);\n",
    "except Exception as e:\n",
    "    Log.error(\"SAVE_CSV\",\"\",\"A generic error occur while saving some entities in csv files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From SQLite to CSV tables (Part 2)\n",
    "<span style=\"color:#9e9e9e\">Exporting WEB_SERVER, ZONE nodes and BELONG,LOCATED, CNAME relationships</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating tables (pandas dataframe) that fully describe each node and relationship type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for NODES: NAME_SERVER(name_id), ZONE (name)\n",
    "query_node_name_server = \"SELECT name_id FROM name_server WHERE name_id IS NOT NULL\";\n",
    "query_node_zone = \"SELECT name FROM zone WHERE name IS NOT NULL AND NOT(length(name)-length(replace(name,'.','')))<2\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for RELATIONSHIPS: BELONG (from NAME_SERVER to ZONE), LOCATED (from NAME_SERVER to NETWORK)\n",
    "query_rel_belong =\"SELECT ns.name_id AS ns_name, z.name AS ns_zone FROM name_server ns INNER JOIN domain_name dn on ns.name_id = dn.string INNER JOIN direct_zone dz on dn.string = dz.domain_name_id INNER JOIN zone z on dz.zone_id = z.name;\"\n",
    "query_rel_located=\"SELECT ns.name_id, a.ip_address_id,iad.ip_network_id FROM name_server ns INNER JOIN domain_name dn on dn.string = ns.name_id INNER JOIN access a on dn.string = a.domain_name_id INNER JOIN ip_address ia on a.ip_address_id = ia.exploded_notation INNER JOIN ip_address_depends iad on ia.exploded_notation = iad.ip_address_id;\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving results in pandas dataframes\n",
    "\n",
    "#NODES:\n",
    "try:\n",
    "    df_node_name_server = pd.read_sql_query(query_node_name_server, connector);\n",
    "except DatabaseError as e:\n",
    "    Log.error(\"NODE\",\"NAME_SERVER\",\"An error occur while loading the current entity. Cannot continue.\");\n",
    "    raise StopExecution;\n",
    "try:\n",
    "    df_node_zone = pd.read_sql_query(query_node_zone, connector);\n",
    "except DatabaseError as e:\n",
    "    Log.error(\"NODE\",\"ZONE\",\"An error occur while loading the current entity. Cannot continue.\");\n",
    "    raise StopExecution;\n",
    "\n",
    "#RELATIONSHIP:\n",
    "try:\n",
    "    df_rel_belong = pd.read_sql_query(query_rel_belong,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"BELONG_NAME_SERVER\",\"Cannot load the current relationship..\");\n",
    "try:\n",
    "    df_rel_located= pd.read_sql_query(query_rel_located,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"LOCATED_NAME_SERVER\",\"Cannot load the current relationship..\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep search for other NAME_SERVER nodes with relative CNAME, BELONG and LOCATED relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for the first search: search from already-founded NAME_SERVER nodes for RR CNAME, obtaining new NAME_SERVER nodes:\n",
    "query_nodes_ns =\"SELECT a.alias_id AS name_id FROM name_server ns INNER JOIN domain_name dn on dn.string = ns.name_id INNER JOIN alias a on dn.string = a.name_id\";\n",
    "query_rel_cnames = \"SELECT ns.name_id AS name_1, a.alias_id AS name_2 FROM name_server ns INNER JOIN domain_name dn on dn.string = ns.name_id INNER JOIN alias a on dn.string = a.name_id\";\n",
    "#Get founded NAME_SERVER nodes and CNAME relationships in 2 lists:\n",
    "init_nodes_ns_list = get_list_from_sqlite(query_nodes_ns,connector,\"NAME_SERVER\");\n",
    "init_rel_cname_list = get_list_from_sqlite(query_rel_cnames,connector,\"CNAME\");\n",
    "#Set also 2 empty lists that will contain LOCATED and BELONG relationships for all the new NAME_SERVER nodes\n",
    "init_rel_belong_list =list();\n",
    "init_rel_located_list=list();\n",
    "#Set a buffer list containing the new NAME_SERVER nodes. In this loop we will deep search all new NAME_SERVER founded from CNAME RR:\n",
    "buffer_list_of_ns_nodes = init_nodes_ns_list.copy();\n",
    "while len(buffer_list_of_ns_nodes)>0:\n",
    "    node= buffer_list_of_ns_nodes.pop();\n",
    "    #From each new NAME_SERVER node, search for another NAME_SERVER node (and CNAME rel) caused by a CNAME RR (searching in alias table)\n",
    "    query_node_ns=\"SELECT a.alias_id AS name_id FROM alias a WHERE a.name_id=\\\"{}\\\";\".format(node[0]);\n",
    "    query_rel_cnames=\"SELECT a.name_id AS name_1, a.alias_id AS name_2 FROM alias a WHERE a.name_id=\\\"{}\\\";\".format(node[0]);\n",
    "    #From each new NAME_SERVER, search for BELONG (from NAME_SERVER to ZONE relationship) (i.e. check in the direct_zone table)\n",
    "    query_rel_belongs = \"SELECT a.alias_id AS ns_name, z.name AS ns_zone FROM alias a INNER JOIN domain_name dn ON a.alias_id=dn.string INNER JOIN direct_zone dz on dn.string = dz.domain_name_id INNER JOIN zone z on dz.zone_id = z.name WHERE a.alias_id=\\\"{}\\\";\".format(node[0]);\n",
    "    #From each new NAME_SERVER, search for LOCATED (from NAME_SERVER to NETWORK relationship) (i.e. check if there's a record in the access table):\n",
    "    query_rel_located =\"SELECT a.alias_id AS name_id, a2.ip_address_id,iad.ip_network_id FROM alias a INNER JOIN domain_name dn on a.alias_id = dn.string INNER JOIN access a2 on dn.string = a2.domain_name_id INNER JOIN ip_address ia on a2.ip_address_id = ia.exploded_notation INNER JOIN ip_address_depends iad on ia.exploded_notation = iad.ip_address_id WHERE a.alias_id=\\\"{}\\\";\".format(node[0]);\n",
    "    #Add founded NAME_SERVER nodes in the buffer list, in order to check in next iterations if there are others CNAME RR:\n",
    "    buffer_list_of_ns_nodes=buffer_list_of_ns_nodes+get_list_from_sqlite(query_node_ns,connector,\"NAME_SERVER\"); #buffer_list_of_ns_nodes.append(get_list_from_sqlite(query_node_ns,connector));\n",
    "    #Filter buffer_list in order to eventually remove empty rows:\n",
    "    buffer_list_of_ns_nodes = list(filter(None, buffer_list_of_ns_nodes));\n",
    "    #Update initial lists containing NAME_SERVER, CNAME,BELONG LOCATED entities:\n",
    "    init_nodes_ns_list=init_nodes_ns_list+ get_list_from_sqlite(query_node_ns,connector,\"NAME_SERVER\");\n",
    "    init_rel_cname_list=init_rel_cname_list + get_list_from_sqlite(query_rel_cnames,connector,\"CNAME\");\n",
    "    init_rel_belong_list=init_rel_belong_list+ get_list_from_sqlite(query_rel_belongs,connector,\"BELONG\");\n",
    "    init_rel_located_list=init_rel_located_list+get_list_from_sqlite(query_rel_located,connector,\"LOCATED\");\n",
    "    \n",
    "#Filter final lists in order to eventually remove empty rows:\n",
    "init_nodes_ns_list = list(filter(None, init_nodes_ns_list));\n",
    "init_rel_cname_list= list(filter(None, init_rel_cname_list));\n",
    "init_rel_belong_list= list(filter(None, init_rel_belong_list));\n",
    "init_rel_located_list= list(filter(None, init_rel_located_list));\n",
    "\n",
    "#Store results in pandas dataframes:\n",
    "df_node_name_server=pd.concat([df_node_name_server,get_data_frame_from_list(init_nodes_ns_list,[\"name_id\"])]);\n",
    "df_rel_cname=get_data_frame_from_list(init_rel_cname_list,[\"name_1\",\"name_2\"]);\n",
    "df_rel_belong =pd.concat([df_rel_belong,get_data_frame_from_list(init_rel_belong_list,[\"ns_name\",\"ns_zone\"])]);\n",
    "\n",
    "\n",
    "df_rel_located=pd.concat([df_rel_located,get_data_frame_from_list(init_rel_located_list,[\"name_id\",\"ip_address_id\",\"ip_network_id\"])]);\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting entities in CSV files (in the directory specified in configuration strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting nodes and relationships in csv files\n",
    "\n",
    "os.makedirs(IMPORT_NEO_FOLDER+CSV_SUB_DIR, exist_ok=True);\n",
    "\n",
    "try:\n",
    "    #Nodes:\n",
    "\n",
    "    df_node_name_server.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/node_name_server.csv\", index=False);\n",
    "    df_node_zone.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/node_zone.csv\", index=False);\n",
    "\n",
    "    #Relationships:\n",
    "\n",
    "    df_rel_belong.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_belong_ns.csv\", index=False);\n",
    "    df_rel_located.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_located_ns.csv\", index=False);\n",
    "    df_rel_cname.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_cname_ns.csv\", index=False);\n",
    "except Exception as e:\n",
    "    Log.error(\"SAVE_CSV\",\"\",\"A generic error occur while saving some entities in csv files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From SQLite to CSV tables (Part 3) \n",
    " <span style=\"color:#9e9e9e\">Exporting AUTONOMOUS_SYSTEM nodes and COMPOSED_BY, DEPEND, MANAGED relationships</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating tables (pandas dataframe) that fully describe each node and relationship type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for NODES: AUTONOMOUS_SYSTEM (number, description, country_code, state)\n",
    "query_node_aut_sys = \"SELECT asy.number,asy.description,\\\"none\\\" AS country_code FROM autonomous_system asy\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for RELATIONSHIPS: COMPOSED_BY (from ZONE to NAME_SERVER), DEPEND (from ZONE to ZONE), MANAGED (from NETWORK to AUTONOMOUS_SYSTEM)\n",
    "query_rel_composed_by =\"SELECT z.name AS zone_name, ns.name_id  AS ns_name FROM zone z INNER JOIN zone_composed zc on z.name = zc.zone_id INNER JOIN name_server ns on ns.name_id = zc.name_server_id WHERE NOT(length(z.name)-length(replace(z.name,'.','')))<2\";\n",
    "#query_rel_depend=\"SELECT z.name, zl.dependency_id FROM zone z INNER JOIN zone_links zl on z.name = zl.zone_id WHERE NOT(z.name=zl.dependency_id) AND NOT(length(z.name)-length(replace(z.name,'.','')))<2\";\n",
    "query_rel_depend=\"SELECT z.name, zl.dependency_id FROM zone z INNER JOIN zone_links zl on z.name = zl.zone_id WHERE NOT(z.name=zl.dependency_id) AND NOT(length(z.name)-length(replace(z.name,'.','')))<2 AND NOT (z.name LIKE '%'||zl.dependency_id AND length(z.name)-length(replace(z.name,'.',''))-length(zl.dependency_id)+length(replace(zl.dependency_id,'.',''))=1)\";\n",
    "query_rel_managed=\"SELECT iad.ip_network_id,asy.number FROM ip_address_depends iad INNER JOIN ip_range_tsv irt on irt.compressed_notation = iad.ip_range_tsv_id INNER JOIN network_numbers nn on irt.compressed_notation = nn.ip_range_tsv_id INNER JOIN autonomous_system asy on asy.number = nn.autonomous_system_id\";\n",
    "\n",
    "query_rel_parent=\"SELECT z.name, zl.dependency_id FROM zone z INNER JOIN zone_links zl on z.name = zl.zone_id WHERE NOT(z.name=zl.dependency_id) AND NOT(length(z.name)-length(replace(z.name,'.','')))<2 AND  z.name LIKE '%'||zl.dependency_id AND length(z.name)-length(replace(z.name,'.',''))-length(zl.dependency_id)+length(replace(zl.dependency_id,'.',''))=1\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving results in pandas dataframes\n",
    "\n",
    "#NODES:\n",
    "try:\n",
    "    df_node_aut_sys= pd.read_sql_query(query_node_aut_sys,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.error(\"NODE\",\"AUT_SYSTEM\",\"An error occur while loading the current entity. Cannot continue.\");\n",
    "    raise StopExecution;\n",
    "\n",
    "#RELATIONSHIP:\n",
    "try:\n",
    "    df_rel_composed_by= pd.read_sql_query(query_rel_composed_by,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"COMPOSED_BY\",\"Cannot load the current relationship..\");\n",
    "try:\n",
    "    df_rel_depend=pd.read_sql_query(query_rel_depend,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"DEPEND\",\"Cannot load the current relationship..\");\n",
    "try:\n",
    "    df_rel_managed=pd.read_sql_query(query_rel_managed,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"MANAGED_BY\",\"Cannot load the current relationship..\");\n",
    "\n",
    "try:\n",
    "    df_rel_parent=pd.read_sql_query(query_rel_parent,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"PARENT\",\"Cannot load the current relationship..\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting entities in CSV files (in the directory specified in config.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting nodes and relationships in csv files\n",
    "\n",
    "os.makedirs(IMPORT_NEO_FOLDER+CSV_SUB_DIR, exist_ok=True);\n",
    "try:\n",
    "    #Nodes\n",
    "\n",
    "    df_node_aut_sys.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/node_aut_sys.csv\", index=False);\n",
    "    print(df_node_aut_sys)\n",
    "\n",
    "    #Relationships\n",
    "\n",
    "    df_rel_composed_by.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_composed_by.csv\", index=False);\n",
    "    df_rel_depend.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_depend.csv\", index=False);\n",
    "    df_rel_managed.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_managed.csv\", index=False);\n",
    "    df_rel_parent.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_parent.csv\", index=False);\n",
    "except Exception as e:\n",
    "    Log.error(\"SAVE_CSV\",\"\",\"A generic error occur while saving some entities in csv files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From SQLite to CSV tables (Part 4) \n",
    " <span style=\"color:#9e9e9e\">Exporting MAIL_DOMAIN, MAIL_SERVER nodes and BELONG, MAPPED_IN, CNAME, LOCATED relationships</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating tables (pandas dataframe) that fully describe each node and relationship type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for NODES: MAIL_DOMAIN(name_id), MAIL_SERVER (name_id)\n",
    "query_node_mail_domain=\"SELECT md.name_id FROM mail_domain md WHERE md.name_id IS NOT NULL;\";\n",
    "query_node_mail_server=\"SELECT ms.name_id FROM mail_server ms WHERE ms.name_id IS NOT NULL;\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for RELATIONSHIPS: \n",
    "query_rel_belong_md =\"SELECT md.name_id AS md_name, z.name AS md_zone FROM mail_domain md LEFT JOIN domain_name dn on md.name_id = dn.string LEFT JOIN direct_zone dz on dn.string = dz.domain_name_id LEFT JOIN zone z on z.name = dz.zone_id WHERE md.name_id IS NOT NULL;\"\n",
    "query_rel_belong_ms=\"SELECT ms.name_id AS ms_name, z.name AS ms_zone FROM mail_server ms LEFT JOIN domain_name dn on ms.name_id = dn.string LEFT JOIN direct_zone dz on dn.string = dz.domain_name_id LEFT JOIN zone z on z.name = dz.zone_id WHERE ms.name_id IS NOT NULL;\";\n",
    "query_rel_mapped_in =\"SELECT md.name_id AS md_name, ms.name_id AS ms_name FROM mail_domain md INNER JOIN mail_domain_composed mdc on md.name_id = mdc.mail_domain_id INNER JOIN mail_server ms on ms.name_id = mdc.mail_server_id;\";\n",
    "query_rel_located =\"SELECT ms.name_id, a.ip_address_id,iad.ip_network_id  FROM mail_server ms INNER JOIN domain_name dn on ms.name_id = dn.string INNER JOIN access a on dn.string = a.domain_name_id INNER JOIN ip_address ia on a.ip_address_id = ia.exploded_notation INNER JOIN ip_address_depends iad on ia.exploded_notation = iad.ip_address_id;\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving results in pandas dataframes\n",
    "\n",
    "#NODES:\n",
    "try:\n",
    "    df_node_mail_domain = pd.read_sql_query(query_node_mail_domain, connector);\n",
    "except DatabaseError as e:\n",
    "    Log.error(\"NODE\",\"MAIL_DOMAIN\",\"An error occur while loading the current entity. Cannot continue.\");\n",
    "    raise StopExecution;\n",
    "try:\n",
    "    df_node_mail_server = pd.read_sql_query(query_node_mail_server, connector);\n",
    "except DatabaseError as e:\n",
    "    Log.error(\"NODE\",\"MAIL_SERVER\",\"An error occur while loading the current entity. Cannot continue.\");\n",
    "    raise StopExecution;\n",
    "\n",
    "#RELATIONSHIP:\n",
    "try:\n",
    "    df_rel_belong_md = pd.read_sql_query(query_rel_belong_md,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"BELONG_MAIL_DOMAIN\",\"Cannot load the current relationship..\");\n",
    "try:\n",
    "    df_rel_belong_ms=pd.read_sql_query(query_rel_belong_ms,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"BELONG_MAIL_SERVER\",\"Cannot load the current relationship..\");\n",
    "try:\n",
    "    df_rel_mapped_in = pd.read_sql_query(query_rel_mapped_in,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"MAPPED_IN\",\"Cannot load the current relationship..\");\n",
    "try:\n",
    "    df_rel_located= pd.read_sql_query(query_rel_located,connector);\n",
    "except DatabaseError as e:\n",
    "    Log.warn(\"REL\",\"LOCATED_MAIL_SERVER\",\"Cannot load the current relationship..\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep search for other MAIL_SERVER nodes with relative CNAME, BELONG, LOCATED relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queries for the first search: search from already-founded MAIL_SERVER nodes for RR CNAME, obtaining new MAIL_SERVER nodes:\n",
    "query_nodes_ms =\"SELECT a.alias_id AS name_id FROM mail_server ms INNER JOIN domain_name dn on dn.string = ms.name_id INNER JOIN alias a on dn.string = a.name_id;\";\n",
    "query_rel_cnames = \"SELECT ms.name_id AS name_1, a.alias_id AS name_2 FROM mail_server ms INNER JOIN domain_name dn on dn.string = ms.name_id INNER JOIN alias a on dn.string = a.name_id\";\n",
    "#Get founded MAIL_SERVER nodes and CNAME relationships in 2 lists:\n",
    "init_nodes_ms_list = get_list_from_sqlite(query_nodes_ms,connector,\"MAIL_SERVER\");\n",
    "init_rel_cname_list = get_list_from_sqlite(query_rel_cnames,connector,\"CNAME\");\n",
    "#Set also 2 empty lists that will contain LOCATED and BELONG relationships for all the new NAME_SERVER nodes\n",
    "init_rel_belong_list =list();\n",
    "init_rel_located_list=list();\n",
    "#Set a buffer list containing the new MAIL_SERVER nodes. In this loop we will deep search all new MAIL_SERVER founded from CNAME RR:\n",
    "buffer_list_of_ms_nodes = init_nodes_ms_list.copy();\n",
    "while len(buffer_list_of_ms_nodes)>0:\n",
    "    node= buffer_list_of_ms_nodes.pop();\n",
    "    #From each new MAIL_SERVER node, search for another MAIL_SERVER node (and CNAME rel) caused by a CNAME RR (searching in alias table)\n",
    "    query_node_ms=\"SELECT a.alias_id AS name_id FROM alias a WHERE a.name_id=\\\"{}\\\";\".format(node[0]);\n",
    "    query_rel_cnames=\"SELECT a.name_id AS name_1, a.alias_id AS name_2 FROM alias a WHERE a.name_id=\\\"{}\\\";\".format(node[0]);\n",
    "    #From each new MAIL_SERVER, search for BELONG (from MAIL_SERVER to ZONE relationship) (i.e. check in the direct_zone table)\n",
    "    query_rel_belongs = \"SELECT a.alias_id AS ms_name, z.name AS ms_zone FROM alias a INNER JOIN domain_name dn ON a.alias_id=dn.string INNER JOIN direct_zone dz on dn.string = dz.domain_name_id INNER JOIN zone z on dz.zone_id = z.name WHERE a.alias_id=\\\"{}\\\";\".format(node[0]);\n",
    "    #From each new MAIL_SERVER, search for LOCATED (from MAIL_SERVER to NETWORK relationship) (i.e. check if there's a record in the access table):\n",
    "    query_rel_located =\"SELECT a.alias_id AS name_id, a2.ip_address_id,iad.ip_network_id FROM alias a INNER JOIN domain_name dn on a.alias_id = dn.string INNER JOIN access a2 on dn.string = a2.domain_name_id INNER JOIN ip_address ia on a2.ip_address_id = ia.exploded_notation INNER JOIN ip_address_depends iad on ia.exploded_notation = iad.ip_address_id WHERE a.alias_id=\\\"{}\\\";\".format(node[0]);\n",
    "    #Add founded MAIL_SERVER nodes in the buffer list, in order to check in next iterations if there are others CNAME RR:\n",
    "    buffer_list_of_ms_nodes.append(get_list_from_sqlite(query_node_ms,connector,\"MAIL_SERVER\"));\n",
    "    #Filter buffer_list in order to eventually remove empty rows:\n",
    "    buffer_list_of_ms_nodes = list(filter(None, buffer_list_of_ms_nodes));\n",
    "    #Update initial lists containing MAIL_SERVER, CNAME,BELONG LOCATED entities:\n",
    "    init_nodes_ms_list=init_nodes_ms_list+ get_list_from_sqlite(query_node_ms,connector,\"MAIL_SERVER\");\n",
    "    init_rel_cname_list=init_rel_cname_list + get_list_from_sqlite(query_rel_cnames,connector,\"CNAME\");\n",
    "    init_rel_belong_list=init_rel_belong_list+ get_list_from_sqlite(query_rel_belongs,connector,\"BELONG\");\n",
    "    init_rel_located_list=init_rel_located_list+get_list_from_sqlite(query_rel_located,connector,\"LOCATED\");\n",
    "\n",
    "#Filter final lists in order to eventually remove empty rows:\n",
    "init_nodes_ms_list = list(filter(None, init_nodes_ms_list));\n",
    "init_rel_cname_list= list(filter(None, init_rel_cname_list));\n",
    "init_rel_belong_list= list(filter(None, init_rel_belong_list));\n",
    "init_rel_located_list= list(filter(None, init_rel_located_list));\n",
    "\n",
    "#Store results in pandas dataframes:\n",
    "df_node_mail_server=pd.concat([df_node_mail_server,get_data_frame_from_list(init_nodes_ms_list,[\"name_id\"])]);\n",
    "df_rel_cname=get_data_frame_from_list(init_rel_cname_list,[\"name_1\",\"name_2\"]);\n",
    "df_rel_belong =pd.concat([df_rel_belong_ms,get_data_frame_from_list(init_rel_belong_list,[\"ms_name\",\"ms_zone\"])]);\n",
    "df_rel_located=pd.concat([df_rel_located,get_data_frame_from_list(init_rel_located_list,[\"name_id\",\"ip_address_id\",\"ip_network_id\"])]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting entities in CSV files (in the directory specified in configuration strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting nodes and relationships in csv files\n",
    "\n",
    "os.makedirs(IMPORT_NEO_FOLDER+CSV_SUB_DIR, exist_ok=True);\n",
    "try:\n",
    "\n",
    "    #Nodes:\n",
    "\n",
    "    df_node_mail_domain.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/node_mail_domain.csv\", index=False);\n",
    "    df_node_mail_server.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/node_mail_server.csv\", index=False);\n",
    "\n",
    "    #Relationships:\n",
    "\n",
    "    df_rel_belong_ms.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_belong_ms.csv\", index=False);\n",
    "    df_rel_belong_md.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_belong_md.csv\", index=False);\n",
    "    df_rel_located.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_located_ms.csv\", index=False);\n",
    "    df_rel_cname.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_cname_ms.csv\", index=False);\n",
    "    df_rel_mapped_in.to_csv(IMPORT_NEO_FOLDER+CSV_SUB_DIR+\"/rel_mapped_in.csv\", index=False);\n",
    "except Exception as e:\n",
    "    Log.error(\"SAVE_CSV\",\"\",\"A generic error occur while saving some entities in csv files.\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
